This raw,  real time motion data  is sent  by these  wireless PCB’s to a computer (a Macintosh Mini  running the  real  time  audio synthesis software SuperCollider), statistically conditioned  for  the  variances  in  motion  and   then   used   to  alter parameters in  a  partially scripted  composition of  vibration, sound  and   light output that  takes  place  under and  around the  visitors’  body.  Since each  of the actuators  is   individually  addressable  through  the   computer  program,  the pressure and  motion data  from  the  participants is used  to  not  only  affect  the overall  intensity of the  sound and  vibration but  also  to  generate specific  and distinct spatial patterns,  for  example,  simple   two  channel pans   (moving the sound between speakers) across  groups of actuators to  more  complex, stochastically determined rhythms and  motions. The visual,  auditory and  haptic perception of vibration, brightness, rate and  rhythm of the flicker-based light, frequency and  amplitude of the  audio in  the  composition’s  three  act  dramatic structure is thus  partially based  on the visitors’ bodily  real time interaction with this sometimes “live,” sensor-augmented floor.
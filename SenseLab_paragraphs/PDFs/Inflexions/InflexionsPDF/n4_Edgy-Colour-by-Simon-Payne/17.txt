The extent  to which  colour  is codified in the realm  of digital media might  seem extraordinarily abstract, but Y'CBCR  processing is in fact analogous to the way in which  luminance and  colour  are  processed by  the  neurophysiology of the  eye and  brain.    In  a  phrase that  resembles a  description of  chroma subsampling Margaret Livingstone explains that,  “in the  retinal ganglion cells the  three  cone signals   [sensitive  to  red,   green   and   blue]   are   transformed  into   two   color- opponent signals...  and  a luminance signal  that  represents the sum  of the activity in all three  cones”  (2002: 88).  Seemingly our means  of perceiving the world have evolved in such a way that it is more  useful  or efficient to analyse luminance and colour  information separately: colour  information contributes to the  recognition of shape and  the  perception of colour  per  se, but  it plays  no  role  at all in the perception of depth, three-dimensionality, movement and  spatial organization, which  are  all cued  by information regarding luminance (Livingstone, 2002: 46). It  is  difficult   to  visualize  or   imagine  the   intricacy  and   complexity  of  the neurological processes associated with  colour  vision,  and an added difficulty that one  might  have  with  a neurological account of perception in general is that  it’s bracketed off from the range  of theories concerning constructive processes or ecological  factors.   The study of neurological activity  does describe certain  effects of particular phenomena, but it doesn’t necessarily account for the particularities of an  experience (Gordon, 2004: 111).   Some  comprehension of the  relationship between neurophysiology and  video  technology is crucial  because the  medium in question has  obviously developed in such  a way  as to take  advantage of the
This vector of distributed and networked intelligence is at work in the current research and development of networked corporations such as Google as it transforms search mechanisms into a much further reaching ever-present, predictive form of AI. As Eric Schmidt has more recently claimed:
“We're still happy to be in search, believe me. But one idea is that more and more searches are done on your behalf without you needing to type…I actually think most people don't want Google to answer their questions, …They want Google to tell them what they should be doing next." (Schmidt quoted in Jenkins, 2010)
In the case of its Prediction API, Google releases its data mining/prediction tool to users on the basis that their data gets stored on Google servers.7  Effectively, what occurs is that data becomes less distributed and more concentrated within the proprietorial grasp and confines of particular networked corporations.  That stored data also becomes the testing field for a tool enabled with machine learning capacities that is likewise Google’s property. This has major implications for networked cultures and for its political economies.  The Prediction API turns out to be a way of initiating a new pathway for, or at least changing, the usual user-developer assemblage in computational culture. Rather than simply providing content (from user) for an application (by developer), the machine learning architecture of the Prediction API, is driven by a recursive adaptation of the data/content by and into the development of the application itself. There are of course precedents here across all kinds of software development communities – gaming and open-source code for example. But machine learning changes the game plan – it automates the development process making it in some fundamental ways nonparticipatory.
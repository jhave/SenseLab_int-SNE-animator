In fact, McCulloch and Pitts shed less light on the brain’s activities and more on a mode of thinking computationally beyond simple input-output models of information processing. Their paper gave impetus to the modeling of artificial rather than biological neural networks and initiated research into questions of modeling learning and adaptation in AI. What this analogy between physical and artificial neurons facilitated was a kind of backwards and forwards mapping that has also entwined and harnessed computational models of thinking to the activity of (idealised) neural correlates: ‘…they simplified and idealized the known properties of networks of neurons so that certain propositional inferences could be mapped onto neural events and vice versa.’ (Piccinini, 2004: 176). Perhaps in spite of the fact that McCulloch and Pitts were aware that the ‘neural’ they were referring to had been largely abstracted from its biology, the far reaching implications of their analogy has meant that computational neural modeling is underpinned by some association to a neurophysiological base. 
 
As Johnston has argued, this enmeshing of the neurobiological with the computational has continued to provide an AI research direction that is different from the concentration upon language, general intelligence and symbolic processing (2008: 386–88). In a range of contemporary AI contexts that deal with large datasets, it is generally acknowledged that various models and manipulations of artificial neural networks provide the best paradigms and applications for machine learning applications that involve pattern recognition. (Luger and Stubblefield, 1998: 759; Joshi et. al., 1997). A deficiency of McCulloch and Pitts’ early neural networks was that the artificial networks modeled were simply too small to compute at either a satisfactory machine-based rate or compare with the operations of the billions of biological neurons that support the generation of actual neural patterning across their networks (Johnston, 2008: 36). In fact, neural networks as they now feature in AI have become largely nonbiological and increasingly concerned and interlinked with branches of statistics and data analysis. This branch of AI’s orientation toward machine learning now finds itself at home in information networks such as the internet and in databases. In part this is due to the fact that large enough datasets and, especially with the development of shared online platforms, enough instances of distributed parallel processing networked nodes can ‘collectively’ combine to create a kind of vast quasi-AI. Or at least this seems to be the dream of networked corporations such as Google, as we can glean from another infamous Eric Schmidt-‘ism’:
In five years, Google will have built "the product I've always wanted to build--we call it 'serendipity,'" he said, adding that it will "tell me what I should be typing." (Schmidt quoted in Mills, 2006)